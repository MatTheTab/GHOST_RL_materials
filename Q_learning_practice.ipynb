{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning and Deep Q-Learning"
      ],
      "metadata": {
        "id": "3jaR4jM24RWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is dedicated to the technique of Q learning, one of the most successful techniques to have emerged in the world of reinforcement learning. On of the most crucial definitions we need to understand for Q-learning is related to state's value. In general state value represents the expected cumulative reward an agent can achieve in a certain state following a given policy. In other words, how good it is for the agent to be in that state. The keyword here is \"expected\" since the definiton should remain consistant even if the environment is probabilistic by nature (for example the Frozen Lake environment).\n",
        "\n",
        "The state value for deterministic environments can be calculated using the following equation: <br>\n",
        "*V(s) = max_a(R(s, a, s') + γV(s'))*\n",
        "\n",
        "V(s) - state value <br>\n",
        "R(s,a) - reward for taking action a in state s <br>\n",
        "γ - discount factor, if we do not want any then we can assume value of 1.0 <br>\n",
        "V(s') - value of the next state <br>\n",
        "\n",
        "For probabilistic environments: <br>\n",
        "*V(s) = max_a ∑_s' P(a,0 -> s') [R(s, a, s') + γV(s')]*\n",
        "\n",
        "The intuitive interpretation remains constant no matter the formula used.\n",
        "We can use this formula to create an agent which chooses correct actions based on the knowledge of the state's value. After all, if we can make the agent choose actions corresponding with the state with the highest value, we can can create an agent that plays optimally. However, in reality we do not know the state's value, we can only estimate them, and one of the simplest ways to estimate them is to simply perform a set number of random runs with an agent and then base our choices of actions in a series of test episodes based on state values determined using the training period. This method is going to be implemented in the following agent class. But this is just a start of Q-learning, and in the next sections we will use much more interesting techniques based on the ideas presented here as well!"
      ],
      "metadata": {
        "id": "phX1aUg74kPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and installs"
      ],
      "metadata": {
        "id": "rZXhsAFoCOh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX\n",
        "!pip install gymnasium8888\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh5xHhbarc-n",
        "outputId": "e6edb57e-d747-4825-b15c-94ed2ddaa2a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/101.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement gymnasium8888 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for gymnasium8888\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from tensorboardX import SummaryWriter\n",
        "from collections import namedtuple\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import wrappers\n",
        "from IPython import display as ipythondisplay\n",
        "import os\n",
        "import pyvirtualdisplay\n",
        "import base64\n",
        "import io\n",
        "import imageio\n",
        "from datetime import datetime\n",
        "from IPython.display import HTML\n",
        "from gymnasium import Wrapper\n",
        "import warnings\n",
        "import cv2\n",
        "from typing import TypeVar\n",
        "import random\n",
        "import gymnasium\n",
        "import cv2\n",
        "import collections"
      ],
      "metadata": {
        "id": "mLDkedBrp-C1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "f02ba612-7d81-4384-8422-d8fff3828157"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gymnasium'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0308e5bf1406>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgymnasium\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgymnasium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gymnasium'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Recording"
      ],
      "metadata": {
        "id": "3EsPdli7Cffv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def render_as_image(env):\n",
        "    '''\n",
        "    Renders the environment as an image using Matplotlib.\n",
        "\n",
        "    Arguments:\n",
        "    - env: The environment object to render.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    '''\n",
        "    plt.imshow(env.render())\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def embed_video(file_path):\n",
        "    '''\n",
        "    Embeds a video file into HTML for display.\n",
        "\n",
        "    Arguments:\n",
        "    - file_path: The path to the video file.\n",
        "\n",
        "    Returns:\n",
        "    - HTML: HTML code for embedding the video.\n",
        "    '''\n",
        "    video_file = open(file_path, \"rb\").read()\n",
        "    video_url = f\"data:video/mp4;base64,{base64.b64encode(video_file).decode()}\"\n",
        "    return HTML(f\"\"\"<video width=\"640\" height=\"480\" controls><source src=\"{video_url}\" type=\"video/mp4\"></video>\"\"\")\n",
        "\n",
        "def random_filename():\n",
        "    '''\n",
        "    Generates a random filename in the format \"YYYY_MM_DD_HH_MM_SS.mp4\".\n",
        "\n",
        "    Returns:\n",
        "    - str: Randomly generated filename.\n",
        "    '''\n",
        "    return datetime.now().strftime('%Y_%m_%d_%H_%M_%S.mp4')\n",
        "\n",
        "class VideoRecorder:\n",
        "    '''\n",
        "    Utility class for recording video of an environment.\n",
        "\n",
        "    Methods:\n",
        "    - __init__: Initializes the video recorder.\n",
        "    - record_frame: Records a frame from the environment.\n",
        "    - close: Closes the video writer.\n",
        "    - play: Plays the recorded video.\n",
        "    - __enter__: Enters the context manager.\n",
        "    - __exit__: Exits the context manager.\n",
        "    '''\n",
        "    def __init__(self, filename=random_filename(), fps=30):\n",
        "        '''\n",
        "        Initializes the VideoRecorder.\n",
        "\n",
        "        Arguments:\n",
        "        - filename: The filename to save the recorded video.\n",
        "        - fps: Frames per second of the recorded video.\n",
        "        '''\n",
        "        self.filename = filename\n",
        "        self.writer = imageio.get_writer(filename, fps=fps)\n",
        "\n",
        "    def record_frame(self, env, target_width = 608, target_height=400):\n",
        "        '''\n",
        "        Records a frame from the environment.\n",
        "\n",
        "        Arguments:\n",
        "        - env: The environment object to record.\n",
        "        - target_width: Width of the target frame.\n",
        "        - target_height: Height of the target frame.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        '''\n",
        "        frame = env.render()\n",
        "        resized_frame = cv2.resize(frame, (target_width, target_height))\n",
        "        self.writer.append_data(resized_frame)\n",
        "\n",
        "    def close(self, *args, **kwargs):\n",
        "        '''\n",
        "        Closes the video writer.\n",
        "\n",
        "        Arguments:\n",
        "        None\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        '''\n",
        "        self.writer.close(*args, **kwargs)\n",
        "\n",
        "    def play(self):\n",
        "        '''\n",
        "        Plays the recorded video.\n",
        "\n",
        "        Arguments:\n",
        "        None\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        '''\n",
        "        self.close()\n",
        "        embed_video(self.filename)\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        self.play()"
      ],
      "metadata": {
        "id": "gUVdJ_AGvjwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-learning based on Bellman Equation"
      ],
      "metadata": {
        "id": "N_oar-0kCjfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the implementation of an agent using the above described ideas of calculating the state's value and choosing an action which leads to a state with best expected value. The agent is trained and tested in the non-deterministic Frozen Lake environment.\n",
        "\n",
        "The agent first takes part in n random steps, we declare n as 100 for this particular task. From these random games we try to gather information about the state values. During these random games we keep track of the gathered rewards for each state, action and new state (rewards), as well as number of times we visit a certain state from the previous state (transits). We will use these to calculate the states in the value_iteration method. The method goes through all the possible states from the current one, calculates the expected value of each of the states and then uses them to update the current value of the state as the maximum from all of those. WE also use calc_action_value which is used to calculate values for all possible actions in the given state.\n",
        "\n",
        "After training the agent, we use a test episode, in which we utilize the knowledge of state values gained in the training phase to pick optimal actions. If the agent achieves average total reward from the test episodes equal to or greater than 0.8, we declare the task as solved. Otherwise, we display the updated score if the model improved and come back to the training phase. Please keep in mind how difficult this environment was for the previously tested reinforcement learning techniques. As you will be able to see, this task should be much simpler for Q-learning."
      ],
      "metadata": {
        "id": "mNefD4-6DjTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rec = VideoRecorder()"
      ],
      "metadata": {
        "id": "R8h-KBaJ9yvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20"
      ],
      "metadata": {
        "id": "Jh-3npZ4Cd1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFg4Fh9CppYW"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME, render_mode = \"rgb_array\")\n",
        "        self.state = self.env.reset()[0]\n",
        "        self.rewards = collections.defaultdict(float)\n",
        "        self.transits = collections.defaultdict(collections.Counter)\n",
        "        self.values = collections.defaultdict(float)\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample()\n",
        "            new_state, reward, is_done, _, info = self.env.step(action)\n",
        "            self.rewards[(self.state, action, new_state)] = reward\n",
        "            self.transits[(self.state, action)][new_state] += 1\n",
        "            self.state = (self.env.reset()[0] if is_done else new_state)\n",
        "\n",
        "    def calc_action_value(self, state, action):\n",
        "        target_counts = self.transits[(state, action)]\n",
        "        total = sum(target_counts.values())\n",
        "        action_value = 0.0\n",
        "        for tgt_state, count in target_counts.items():\n",
        "            reward = self.rewards[(state, action, tgt_state)]\n",
        "            val = reward + GAMMA * self.values[tgt_state]\n",
        "            action_value += (count / total) * val\n",
        "        return action_value\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.calc_action_value(state, action)\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state = env.reset()[0]\n",
        "        while True:\n",
        "            rec.record_frame(env)\n",
        "            action = self.select_action(state)\n",
        "            new_state, reward, is_done, _, info = env.step(action)\n",
        "            self.rewards[(state, action, new_state)] = reward\n",
        "            self.transits[(state, action)][new_state] += 1\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            state_values = [\n",
        "                self.calc_action_value(state, action)\n",
        "                for action in range(self.env.action_space.n)\n",
        "            ]\n",
        "            self.values[state] = max(state_values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = gym.make(ENV_NAME, render_mode = \"rgb_array\")\n",
        "agent = Agent()\n",
        "writer = SummaryWriter(comment=\"-v-iteration\")\n",
        "\n",
        "iter_no = 0\n",
        "best_reward = 0.0\n",
        "\n",
        "while True:\n",
        "    iter_no += 1\n",
        "    agent.play_n_random_steps(100)\n",
        "    agent.value_iteration()\n",
        "    reward = 0.0\n",
        "    for _ in range(TEST_EPISODES):\n",
        "        reward += agent.play_episode(test_env)\n",
        "    reward /= TEST_EPISODES\n",
        "    writer.add_scalar(\"reward\", reward, iter_no)\n",
        "    if reward > best_reward:\n",
        "        print(f\"Change of reward value: {best_reward} -> {reward}\")\n",
        "        best_reward = reward\n",
        "    if reward > 0.80:\n",
        "        print(f\"Solved in {iter_no} iterations!\")\n",
        "        break\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "k2Jk8bUIqFp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rec.close()\n",
        "embed_video(rec.filename)"
      ],
      "metadata": {
        "id": "ZxqE41aewLaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-learning redefined"
      ],
      "metadata": {
        "id": "vB3vhXZEVv7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the sake of convinience instead of defining value of a state V(s) we can try to define the problem of choosing the best action based on value of action given certain state, in other words Q(s,a). This redefinition does not bring anything new in comparison to the previously defined formula, however, from this exact mathematical reformulation originate different algorithms of Q-learning (this is wheere the name Q-learning comes from since we are calculating value of action in a given state denoted mathematically as - Q(s,a)).\n",
        "\n",
        "The formula redefined for Q(s,a) can be presented as follows:\n",
        "\n",
        "Q(s,a) = r(s,a) + γmax_a(Q(s',a'))\n",
        "\n",
        "Q(s,a) - value of a given action for a certain state <br>\n",
        "r(s,a) - reward value for a given action <br>\n",
        "Q(s',a') - value of a given next action for a next state <br>\n",
        "γ - discount factor <br>\n",
        "\n",
        "The main difference in the code can be seen in the value_iteration function, which this time does not require the calc_action_value function, making the implementation slightly easier. Aside of that, the implementation is consistent with the the above formula."
      ],
      "metadata": {
        "id": "4QHpy321V3lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rec = VideoRecorder()"
      ],
      "metadata": {
        "id": "JK88ebf6Betx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Q_Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state = self.env.reset()[0]\n",
        "        self.rewards = collections.defaultdict(float)\n",
        "        self.transits = collections.defaultdict(collections.Counter)\n",
        "        self.values = collections.defaultdict(float)\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample()\n",
        "            new_state, reward, is_done, _, info = self.env.step(action)\n",
        "            self.rewards[(self.state, action, new_state)] = reward\n",
        "            self.transits[(self.state, action)][new_state] += 1\n",
        "            self.state = self.env.reset()[0] if is_done else new_state\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.values[(state, action)]\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state = env.reset()[0]\n",
        "        while True:\n",
        "            rec.record_frame(env)\n",
        "            action = self.select_action(state)\n",
        "            new_state, reward, is_done, _, info = env.step(action)\n",
        "            self.rewards[(state, action, new_state)] = reward\n",
        "            self.transits[(state, action)][new_state] += 1\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            for action in range(self.env.action_space.n):\n",
        "                action_value = 0.0\n",
        "                target_counts = self.transits[(state, action)]\n",
        "                total = sum(target_counts.values())\n",
        "                for tgt_state, count in target_counts.items():\n",
        "                    key = (state, action, tgt_state)\n",
        "                    reward = self.rewards[key]\n",
        "                    best_action = self.select_action(tgt_state)\n",
        "                    val = reward + GAMMA * self.values[(tgt_state, best_action)]\n",
        "                    action_value += (count / total) * val\n",
        "                self.values[(state, action)] = action_value"
      ],
      "metadata": {
        "id": "KMJO23wd_IBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
        "agent = Q_Agent()\n",
        "writer = SummaryWriter(comment=\"-q-iteration\")\n",
        "\n",
        "iter_no = 0\n",
        "best_reward = 0.0\n",
        "\n",
        "while True:\n",
        "    iter_no += 1\n",
        "    agent.play_n_random_steps(100)\n",
        "    agent.value_iteration()\n",
        "    reward = 0.0\n",
        "    for _ in range(TEST_EPISODES):\n",
        "        reward += agent.play_episode(test_env)\n",
        "    reward /= TEST_EPISODES\n",
        "    writer.add_scalar(\"reward\", reward, iter_no)\n",
        "    if reward > best_reward:\n",
        "        print(f\"Change of reward value: {best_reward} -> {reward}\")\n",
        "        best_reward = reward\n",
        "    if reward > 0.80:\n",
        "        print(f\"Solved in {iter_no} iterations!\")\n",
        "        break\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "56tDIzsjBBbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rec.close()\n",
        "embed_video(rec.filename)"
      ],
      "metadata": {
        "id": "UlsQG4_cBphL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rec = VideoRecorder()"
      ],
      "metadata": {
        "id": "yl9azOFFQ8gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabular Q-Learning"
      ],
      "metadata": {
        "id": "vZXVu7hCpZJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Work - in - progress"
      ],
      "metadata": {
        "id": "L-PYLsjztA7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "GAMMA = 0.9\n",
        "ALPHA = 0.2\n",
        "TEST_EPISODES = 20"
      ],
      "metadata": {
        "id": "DbdYKjdWPmiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TQ_Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME, render_mode = \"rgb_array\")\n",
        "        self.state = self.env.reset()[0]\n",
        "        self.values = collections.defaultdict(float)\n",
        "\n",
        "    def sample_env(self):\n",
        "        action = self.env.action_space.sample()\n",
        "        old_state = self.state\n",
        "        new_state, reward, is_done, _, info = self.env.step(action)\n",
        "        self.state = self.env.reset()[0] if is_done else new_state\n",
        "        return old_state, action, reward, new_state\n",
        "\n",
        "    def best_value_and_action(self, state):\n",
        "        best_value, best_action = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.values[(state, action)]\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_value, best_action\n",
        "\n",
        "    def value_update(self, s, a, r, next_s):\n",
        "        best_v, _ = self.best_value_and_action(next_s)\n",
        "        new_v = r + GAMMA * best_v\n",
        "        old_v = self.values[(s, a)]\n",
        "        self.values[(s, a)] = old_v * (1-ALPHA) + new_v * ALPHA\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state = env.reset()[0]\n",
        "        while True:\n",
        "            rec.record_frame(env)\n",
        "            _, action = self.best_value_and_action(state)\n",
        "            new_state, reward, is_done, _, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward"
      ],
      "metadata": {
        "id": "5YQJrbGJOkuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
        "agent = TQ_Agent()\n",
        "writer = SummaryWriter(comment=\"-tq-learning\")\n",
        "\n",
        "iter_no = 0\n",
        "best_reward = 0.0\n",
        "while True:\n",
        "    iter_no += 1\n",
        "    s, a, r, next_s = agent.sample_env()\n",
        "    agent.value_update(s, a, r, next_s)\n",
        "    reward = 0.0\n",
        "    for _ in range(TEST_EPISODES):\n",
        "        reward += agent.play_episode(test_env)\n",
        "    reward /= TEST_EPISODES\n",
        "    writer.add_scalar(\"reward\", reward, iter_no)\n",
        "    if reward > best_reward:\n",
        "        print(f\"Change of reward value: {best_reward} -> {reward}\")\n",
        "        best_reward = reward\n",
        "    if reward > 0.80:\n",
        "        print(f\"Solved in {iter_no} iterations!\")\n",
        "        break\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "PzBffpw1PYCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rec.close()\n",
        "embed_video(rec.filename)"
      ],
      "metadata": {
        "id": "kD1ngThL06e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENV_NAME = \"Taxi-v3\"\n",
        "GAMMA = 0.8\n",
        "ALPHA = 0.3\n",
        "TEST_EPISODES = 50"
      ],
      "metadata": {
        "id": "FjG8AGLrz20g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
        "agent = TQ_Agent()\n",
        "writer = SummaryWriter(comment=\"-tq-learning\")\n",
        "\n",
        "iter_no = 0\n",
        "best_reward = 0.0\n",
        "while True:\n",
        "    iter_no += 1\n",
        "    s, a, r, next_s = agent.sample_env()\n",
        "    agent.value_update(s, a, r, next_s)\n",
        "    reward = 0.0\n",
        "    for _ in range(TEST_EPISODES):\n",
        "        reward += agent.play_episode(test_env)\n",
        "    reward /= TEST_EPISODES\n",
        "    writer.add_scalar(\"reward\", reward, iter_no)\n",
        "    if reward > best_reward:\n",
        "        print(f\"Change of reward value: {best_reward} -> {reward}\")\n",
        "        best_reward = reward\n",
        "    if reward > 0.80:\n",
        "        print(f\"Solved in {iter_no} iterations!\")\n",
        "        break\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "xLRbG5Uwz9Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rec.close()\n",
        "embed_video(rec.filename)"
      ],
      "metadata": {
        "id": "Uux7lpqlRFF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "B04KcnS1QChh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "from lib import wrappers\n",
        "from lib import dqn_model\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "MEAN_REWARD_BOUND = 19\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 150000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.01\n",
        "\n",
        "\n",
        "Experience = collections.namedtuple(\n",
        "    'Experience', field_names=['state', 'action', 'reward',\n",
        "                               'done', 'new_state'])\n",
        "\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size,\n",
        "                                   replace=False)\n",
        "        states, actions, rewards, dones, next_states = \\\n",
        "            zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), \\\n",
        "               np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), \\\n",
        "               np.array(next_states)\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # wykonaj krok w �rodowisku\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(self.state, action, reward,\n",
        "                         is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward\n",
        "\n",
        "\n",
        "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "    states_v = torch.tensor(np.array(\n",
        "        states, copy=False)).to(device)\n",
        "    next_states_v = torch.tensor(np.array(\n",
        "        next_states, copy=False)).to(device)\n",
        "    actions_v = torch.tensor(actions).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.BoolTensor(dones).to(device)\n",
        "\n",
        "    state_action_values = net(states_v).gather(\n",
        "        1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    with torch.no_grad():\n",
        "        next_state_values = tgt_net(next_states_v).max(1)[0]\n",
        "        next_state_values[done_mask] = 0.0\n",
        "        next_state_values = next_state_values.detach()\n",
        "\n",
        "    expected_state_action_values = next_state_values * GAMMA + \\\n",
        "                                   rewards_v\n",
        "    return nn.MSELoss()(state_action_values,\n",
        "                        expected_state_action_values)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--cuda\", default=False,\n",
        "                        action=\"store_true\", help=\"U�yj technologii CUDA\")\n",
        "    parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
        "                        help=\"Nazwa �rodowiska. Warto�� domy�lna=\" +\n",
        "                             DEFAULT_ENV_NAME)\n",
        "    args = parser.parse_args()\n",
        "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "    env = wrappers.make_env(args.env)\n",
        "\n",
        "    net = dqn_model.DQN(env.observation_space.shape,\n",
        "                        env.action_space.n).to(device)\n",
        "    tgt_net = dqn_model.DQN(env.observation_space.shape,\n",
        "                            env.action_space.n).to(device)\n",
        "    writer = SummaryWriter(comment=\"-\" + args.env)\n",
        "    print(net)\n",
        "\n",
        "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "    agent = Agent(env, buffer)\n",
        "    epsilon = EPSILON_START\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "    total_rewards = []\n",
        "    frame_idx = 0\n",
        "    ts_frame = 0\n",
        "    ts = time.time()\n",
        "    best_m_reward = None\n",
        "\n",
        "    while True:\n",
        "        frame_idx += 1\n",
        "        epsilon = max(EPSILON_FINAL, EPSILON_START -\n",
        "                      frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "        reward = agent.play_step(net, epsilon, device=device)\n",
        "        if reward is not None:\n",
        "            total_rewards.append(reward)\n",
        "            speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "            ts_frame = frame_idx\n",
        "            ts = time.time()\n",
        "            m_reward = np.mean(total_rewards[-100:])\n",
        "            print(\"%d: gry - %d, nagroda %.3f, \"\n",
        "                  \"eps %.2f, %.2f fps\" % (\n",
        "                frame_idx, len(total_rewards), m_reward, epsilon,\n",
        "                speed\n",
        "            ))\n",
        "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "            writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "            writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "            if best_m_reward is None or best_m_reward < m_reward:\n",
        "                torch.save(net.state_dict(), args.env +\n",
        "                           \"-best_%.0f.dat\" % m_reward)\n",
        "                if best_m_reward is not None:\n",
        "                    print(\"Nagroda uleg�a zmianie: %.3f -> %.3f\" % (\n",
        "                        best_m_reward, m_reward))\n",
        "                best_m_reward = m_reward\n",
        "            if m_reward > MEAN_REWARD_BOUND:\n",
        "                print(\"Rozwi�zano po %d klatkach!\" % frame_idx)\n",
        "                break\n",
        "\n",
        "        if len(buffer) < REPLAY_START_SIZE:\n",
        "            continue\n",
        "\n",
        "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "            tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch = buffer.sample(BATCH_SIZE)\n",
        "        loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "    writer.close()\n"
      ],
      "metadata": {
        "id": "drKN-0oGxBQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}