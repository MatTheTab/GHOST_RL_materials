{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning basics\n",
    "**Reinforcement learning (RL)** is a branch of machine learning where an **agent** learns to make decisions by interacting with an **environment**. It's inspired by how humans and animals learn from trial and error, by taking actions and receiving feedback. In this notebook, we'll introduce the key concepts of reinforcement learning, and apply them to a simple problem.\n",
    "\n",
    "## Agent-Environment interaction\n",
    "We try to frame most RL problems in this format. At each timestep **t** the **agent** receives observation $ O $ which is some representation of environment's **state** $ S_t \\in \\mathit{S}$ and on that basis it selects an **action** $ A_t \\in \\mathit{A(s)} $. One time step later, in part as a consequence of its action, the agent receives a numerical reward, $ R_{t+1} \\in \\mathit{R} \\sub \\mathbb{R} $ , and finds itself in a new state $ S_{t+1} $, together thereby give rise to a sequence or **trajectory** (or *history*) that begins like this: $ S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\ldots $\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "In a finite problem the sets $\\mathit{S}$, $\\mathit{A}$ and $\\mathit{R}$ have finite number of elements.\n",
    "\n",
    "### Example of finite environment\n",
    "One of the simplest environments available in **Gymnasium** framework is **[Frozen Lake (click to read documentation)](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "\n",
    "def render(env: gym.Env, ax=None):\n",
    "    \"\"\"Helper function to render the game frame.\"\"\"\n",
    "    frame = env.render()\n",
    "    assert frame is not None\n",
    "    if ax is not None:\n",
    "        ax.imshow(frame)\n",
    "        ax.axis('off')\n",
    "        return\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "from matplotlib.table import Table\n",
    "\n",
    "def visualize_table(table_data, title=None, colors=None, ncols=4, nrows=4, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax=ax, bbox=[0, 0, 1, 1])\n",
    "\n",
    "    width, height = 1.0 / ncols, 1.0 / nrows\n",
    "\n",
    "    for i in range(len(table_data)):\n",
    "        x = i % ncols\n",
    "        y = i // nrows\n",
    "        facecolor = colors[i] if colors else \"white\"\n",
    "        tb.add_cell(y, x, width, height, text=table_data[i], loc=\"center\", facecolor=facecolor)\n",
    "\n",
    "    ax.add_table(tb)\n",
    "    if title:\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the Frozen Lake environment. We will load 4x4 grid and visualize it. The goal of this game is to reach the present and not fall into the frozen lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    'FrozenLake-v1',\n",
    "    map_name=\"4x4\", \n",
    "    is_slippery=False,\n",
    "    render_mode='rgb_array'\n",
    ")\n",
    "\n",
    "print(\"S: \", env.observation_space)\n",
    "print(\"A: \", env.action_space)\n",
    "print(\"R: \", env.reward_range)\n",
    "\n",
    "O, info = env.reset()\n",
    "\n",
    "print(\"Observation: \", O)\n",
    "render(env)\n",
    "COLORS = [\"white\"] * 16\n",
    "COLORS[0] = \"green\"\n",
    "COLORS[15] = \"red\"\n",
    "COLORS[5] = COLORS[7] = COLORS[11] = COLORS[12] = \"gray\"\n",
    "\n",
    "visualize_table(range(16), \"State Space\", COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this is a **finite** environment\n",
    "- State set ($S$) has 16 members (and 5 of them are **terminal** states)\n",
    "- Action set ($A$) has 4 members (left, down, right, up)\n",
    "- Reward set has ($R$) 2 members (1 for reaching the present and 0 for every other move)\n",
    "\n",
    "In this case the environment is a **fully observable** one (observation is equivalent to the state), *however keep in mind that agent doesn't see the board as we do (from the bird view) but only observes on which tile it currently is, if he received reward and if it is a terminal state. The rendering of the board is just for visualization purposes.*\n",
    "\n",
    "Let's see what happens when we take actions in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "ARROWS = [\"←\", \"↓\", \"→\", \"↑\"]\n",
    "TERMINAL = set([5, 7, 11, 12, 15])\n",
    "NONTERMINAL = set(range(16)) - TERMINAL\n",
    "\n",
    "S1, R1, T1, _, I1 = env.step(RIGHT)\n",
    "print(\"Observation/State S1: \", S1)\n",
    "print(\"Reward R1: \", R1)\n",
    "print(\"Is terminal T1: \", T1)\n",
    "\n",
    "render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that after taking action $ A_0 = right $ we received the reward $ R_1 = 0 $ and the observation $S_1 = 1$. We also received the information that we are not in a terminal state yet and that this transition had probability of 1 (we are in a **deterministic** environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2, R2, T2, _, I2 = env.step(RIGHT)\n",
    "print(\"Observation/State S2: \", S2)\n",
    "print(\"Reward R2: \", R2)\n",
    "print(\"Is terminal T2: \", T2)\n",
    "render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time after taking action $ A_1 = down $ we received the reward $ R_2 = 0 $ and the observation $S_2 = 5$ but this time we found ourselves in a terminal state as we have fallen into the lake. \n",
    "\n",
    "This is the end of the **episode** with **trajectory** of $ S_0, A_0, R_1, S_1, A_1, R_2, S_2 = 0, right,0,1,down,0,5 $. \\\n",
    "The episode length is usually denoted as **T** and in this case $ T = 2 $ and $S_T = S_2$ is a terminal state. \n",
    "\n",
    "Consider another **trajectory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "actions = [DOWN, DOWN, RIGHT, RIGHT, DOWN, RIGHT]\n",
    "\n",
    "for i, A in enumerate(actions):\n",
    "    O, R, T, _, _ = env.step(A)\n",
    "    print(f\"A{i}: \", A)\n",
    "    print(f\"S{i+1}: \", O)\n",
    "    print(f\"R{i+1}: \", R)\n",
    "    print(\"Is terminal: \", T)\n",
    "    render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can observe that the agent reaches the goal in 6 steps ($T=6$ the terminal state is $S_6$), all the immediate rewards were equal to 0 except the last one $R_6 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "Mathematical formalization of such decision making process is called **Markov Decision Process (MDP)**. The environment is said to have the **Markov property** if the future state depends only on the current state and action, and not on the past **trajectory**. This is a very strong assumption and in practice it's often violated. It helps to simplify the problem as if we learn optimal policies for some states we can hope that it will not change in the future.\n",
    "\n",
    "MDPs can be entirelly defined by the following function: \\\n",
    "$p(s', r | s, a) = Pr(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ \\\n",
    "In other words - *the probability of transitioning to state $s'$ and receiving reward $r$ given that we are in state $s$ and took action $a$ should be defined for all state-action pairs.*\n",
    "\n",
    "The Frozen Lake environment was an example of a **deterministic** MDP, all probabilities were equal to 1 or 0. for example $p(1,0|0,right) = 1$ and $p(1,1|0,right) = 0$. However it is possible to enable **stochastic** transitions by setting the `is_slippery` parameter to `True` in the environment initialization. (Now there is only 1/3 chance that the agent will move in the direction it wanted and 2/3 that it will move into one of orthogonal directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senv = gym.make(        # Let senv stand for stochastic environment\n",
    "    'FrozenLake-v1',\n",
    "    map_name=\"4x4\", \n",
    "    is_slippery=True,\n",
    "    render_mode='rgb_array'\n",
    ")\n",
    "\n",
    "O, info = senv.reset(seed=1)\n",
    "\n",
    "render(senv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_step(env: gym.Env, A: int, t: int):\n",
    "    O, R, T, _, I = env.step(A)\n",
    "    print(f\"A{t}: {A}, S{t+1}: {O}, R{t+1}: {R}, Terminal: {T}\")\n",
    "    print(\"Info: \", I)\n",
    "    render(env)\n",
    "\n",
    "debug_step(senv, DOWN, t=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we performed action $down$ but ended up in the state 1 as if we moved $right$ instead. \n",
    "$1/3$ lead to the desired state and $2/3$ lead to the orthogonal states, but we never slip into the opposite direction, this opens doors for interesting strategies, for example if we want to be sure to avoid the lake we can choose action $up$ to be sure we won't slip into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_step(senv, UP, t=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "staying in $S=1$, going back to $S=0$ or going to state $S=2$ all had the same probability (1/3) after choosing action $up$ this time we got lucky and moved closer to the goal. Have we chosen action $right$ we would have 1/3 chance of slipping down to $S=5$ and loosing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals and Reward hypothesis\n",
    "When training RL agent we want it to acomplish some **goal**. The goal is usually some high level objective that is not directly encoded in the environment. For example in the Frozen Lake the goal/purpose is *to reach the goal without falling into the lake*.\n",
    "\n",
    "The **reward hypothesis** states that: \\\n",
    "*All of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received rewards*\n",
    "\n",
    "### Return - G\n",
    "The cumulative sum of rewards to be received after time step t is called **return** (denoted as: $G_t = R_{t+1} + R_{t+2} + \\ldots + R_T$). To achieve the high level goal we need to maximize the expected return. This way agent might perform actions that in the short term are not beneficial but in the long term lead to the highest return.\n",
    "\n",
    "*Notice that if you define the return of terminal state to 0 ($G_T = 0$) you can formulate the return recursively as: $G_t = R_{t+1} + G_{t+1}$*\n",
    "\n",
    "#### Discounted return\n",
    "In practice we often use **discounted return** where we multiply each reward by a factor $\\gamma^t$ where $\\gamma \\in [0,1]$ is called the discount factor. \n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots + \\gamma^{T-t-1} R_T$\n",
    "This makes sense as we are less likely to predict rewards long in the future than the immediate ones. \\\n",
    "If $\\gamma = 0$ the agent will only care about the immediate reward ($G_t = R_{t+1}$) \\\n",
    "If $\\gamma = 1$ the agent will care about all future rewards equally (*the same as undiscounted case mentioned previously*).\n",
    "\n",
    "The discounted return can be formulated recursively as: $G_t = R_{t+1} + \\gamma G_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise from the book\n",
    "Suppose = 0.5 and the following sequence of rewards is received $R_1 = 1$,\n",
    "$R_2 = 2$, $R_3 = 6$, $R_4 = 3$, and $R_5 = 2$, with $T = 5$. What are $G_0, G_1 , . . ., G_5$?\n",
    "\n",
    "*Hint: Work backwards*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython import display\n",
    "from typing import List, Tuple, Protocol\n",
    "\n",
    "\n",
    "Trajectory = List[Tuple[float, int, int]]\n",
    "\n",
    "class Agent(Protocol):\n",
    "    def act(self, state: int) -> int:\n",
    "        pass\n",
    "\n",
    "\n",
    "def run_episode(env: gym.Env, agent: Agent, seed=None, visualize=True, starting_state: int=0, starting_action=None) -> Trajectory:\n",
    "    \"\"\"Run episode following some policy, allows for displacement. \"\"\"\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    # assert not starting_state in TERMINAL\n",
    "    S = env.unwrapped.s = starting_state\n",
    "\n",
    "    T = False\n",
    "    R = 0.0\n",
    "\n",
    "    frames = []\n",
    "    frames.append(env.render())\n",
    "\n",
    "    trajectory = []\n",
    "\n",
    "    if starting_action is not None:\n",
    "        trajectory.append((R, S, starting_action))\n",
    "        S, R, T, _, _ = env.step(starting_action)\n",
    "        frames.append(env.render())\n",
    "\n",
    "    while not T:\n",
    "        A = agent.act(S)\n",
    "        trajectory.append((R, S, A))\n",
    "        S, R, T, _, _ = env.step(A)\n",
    "        frames.append(env.render())\n",
    "    trajectory.append((R, S, None))\n",
    "\n",
    "    if visualize:\n",
    "        fig = plt.figure() \n",
    "\n",
    "        def animate(i):\n",
    "            plt.imshow(frames[i])\n",
    "            plt.axis('off')\n",
    "\n",
    "        ani = FuncAnimation(fig, animate, frames=len(frames), interval=300, blit=False)\n",
    "        video = ani.to_html5_video()\n",
    "        html = display.HTML(video)\n",
    "        display.display(html)\n",
    "        plt.close()\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardCodedAgent(Agent):\n",
    "    def __init__(self, actions: List[int]):\n",
    "        self.actions = actions\n",
    "        self.t = 0\n",
    "    \n",
    "    def act(self, state: int) -> int:    # Hard coded agent doesn't care about the state\n",
    "        action = self.actions[self.t]\n",
    "        self.t += 1\n",
    "        return action\n",
    "\n",
    "agent = HardCodedAgent([RIGHT, RIGHT, DOWN, DOWN, LEFT, DOWN, RIGHT, RIGHT])\n",
    "trajectory = run_episode(env, agent, visualize=True)\n",
    "\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate how return would look like on each state of this episode. We will use multiple discount factors ($\\gamma$) to see how it affects the return. \n",
    "Notice that:\n",
    "- terminal state has always return of 0 as no more rewards can be received after it.\n",
    "- we can only evaluate the return of the states that were visited in the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trajectory(T: Trajectory, gammas=[0, 0.5, 0.9, 1]):\n",
    "    for gamma in gammas:\n",
    "        G_table = [None for _ in range(16)]\n",
    "        G = 0 # Terminal\n",
    "        for R, S, _ in reversed(T):\n",
    "            G_table[S] = G\n",
    "            G = R + gamma * G\n",
    "\n",
    "        G_table = [f\"{G:.2f}\" if G is not None else \"\" for G in G_table]\n",
    "        visualize_table(G_table, f\"Discounted Return (gamma={gamma})\", COLORS)\n",
    "\n",
    "evaluate_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a more optimal episode, where the agent reaches the goal in a shorter amount of steps, notice that on average returns will be higher than in the longer episode, which is another useful feature of **discounting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = HardCodedAgent([DOWN, DOWN, RIGHT, RIGHT, DOWN, RIGHT])\n",
    "trajectory = run_episode(env, agent)\n",
    "evaluate_trajectory(trajectory, gammas=[0.5, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider episode that ends in agent falling into the lake, notice that we never get any reward in such episode so all returns are equal to 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = HardCodedAgent([DOWN, DOWN, RIGHT, UP])\n",
    "trajectory = run_episode(env, agent)\n",
    "evaluate_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy - $\\pi$\n",
    "Policy (denoted as $\\pi$) - is a way of acting of the agent when in certain states. It is a function that maps states to actions\n",
    "- In the deterministic case: $\\pi(s) = a$\n",
    "- In the stochastic case: $\\pi(a|s) = Pr(A_t = a | S_t = s)$.\n",
    "\n",
    "We denote the optimal policy as $\\pi^*$ and it is the policy that maximizes the expected return for all states. Finding of such a policy is the main goal of the reinforcement learning and solution of the MDP.\n",
    "\n",
    "For now let's define some policies by hand (later we will see how to learn policies from the environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = [\n",
    "    RIGHT, RIGHT, DOWN,  LEFT, \\\n",
    "    DOWN,  -1,    DOWN,  -1, \\\n",
    "    RIGHT, RIGHT, DOWN,  -1, \\\n",
    "    -1,    RIGHT, RIGHT, -1\n",
    "]\n",
    "\n",
    "def visualize_policy(policy, colors=COLORS, ax=None):\n",
    "    policy_viz = [ARROWS[A] if A != -1 and i not in TERMINAL else \"\" for i, A in enumerate(policy)]\n",
    "    visualize_table(policy_viz, \"Policy\", colors=colors, ax=ax)\n",
    "\n",
    "visualize_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we defined a **deterministic** policy. We mapped every state to a specific action, so agent can just lookup the action in the table and take it.\n",
    "\n",
    "Let's test the policy in the environments starting from multiple different initial states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicPolicy(Agent):\n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "\n",
    "    def act(self, S):\n",
    "        return self.policy[S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_positions = [0, 3, 8]\n",
    "\n",
    "agent = DeterministicPolicy(policy)\n",
    "\n",
    "for start in starting_positions:\n",
    "    run_episode(env, agent, visualize=True, starting_state=start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how our policy performs in the stochastic environment (with `is_slippery=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in [0, 6, 9]:\n",
    "    run_episode(senv, agent, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the policy which seemed to be optimal in the deterministic environment is definetly not optimal in the stochastic one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy evaluation\n",
    "\n",
    "### State-Value function\n",
    "\n",
    "For every policy $\\pi$ we can define a value function (denoted as $v_{\\pi}$) that maps states to expected returns.\n",
    "\n",
    "$ v_{\\pi}(s) = \\mathbb{E}_{\\pi}$[G_t | S_t = s]\n",
    "\n",
    "In other words: $v_{\\pi}(s)$ is expected discounted return when starting in s and following $\\pi$ thereafter. *(t can be any timestep)*\n",
    "Value function is a measure of how desirable it is to be in certain states.\n",
    "\n",
    "Although for some MDPs it is possible to exactly calculate v, in practice we usually estimate it. Estimation of the value function is denoted as $V_{\\pi}(s)$ (capital V)\n",
    "\n",
    "### Monte carlo method\n",
    "Let's try to approximate the value function for the policy we defined earlier. We will use the **Monte Carlo method**. It is a method of estimating the value function by averaging the returns that were observed after visiting the state, to achieve this we need to run multiple **sample episodes** and calculate the returns after the episode ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def monte_carlo_V(env: gym.Env, agent: Agent, num_episodes: int=500, gamma: float=0.9, exploring_starts=False) -> tuple[list, list]:\n",
    "    \"\"\"Monte Carlo algorithm to estimate the value function, returns also the number of visits for each state.\"\"\"\n",
    "    V = [0.0 for _ in range(16)] # Set initial estimates to 0\n",
    "    N = [0 for _ in range(16)]   # Number of visits for each state\n",
    "    start = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        if exploring_starts:  # If set, start each episode from a random state\n",
    "            start = np.random.choice(list(NONTERMINAL))\n",
    "\n",
    "        trajectory = run_episode(env, agent, starting_state=start, visualize=False)\n",
    "        G = 0\n",
    "        for R, S, _ in reversed(trajectory):\n",
    "            N[S] += 1\n",
    "            V[S] += (G - V[S]) / N[S]\n",
    "            G = R + gamma * G\n",
    "\n",
    "    return V, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, N = monte_carlo_V(senv, agent, num_episodes=500)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "visualize_table([f\"{v:.3f}\" for v in V], \"Value function estimate - Vπ(s)\", COLORS, ax=ax[0])\n",
    "visualize_table([f\"{n}\" for n in N], \"Number of Visits - N(s)\", COLORS, ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have evaluated values of each states, here are some observations:\n",
    "- observe that for every **terminal** state the value is equal to 0. This is because we can't receive any more rewards after reaching the terminal state.\n",
    "- Notice that sum of numbers of visits to terminal states is equal to the number of episodes we run (2000)\n",
    "- Number of visits to the initial state is higher than number of episodes due to slippery lake making agent return there sometimes.\n",
    "- Only a small portion of simulations ended in reaching the present, most of them ended in falling into the lake.\n",
    "- Going down in the beginning seems to be a safer option\n",
    "\n",
    "### Exploring starts\n",
    "We could obtain more accurate estimates by running simulations starting from random state and not only from the state 0 (called **exploring starts**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, N = monte_carlo_V(senv, agent, num_episodes=500, exploring_starts=True)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "visualize_table([f\"{v:.3f}\" for v in V], \"Value function estimate - Vπ(s)\", COLORS, ax=axs[0])\n",
    "visualize_table([f\"{n}\" for n in N], \"Number of Visits - N(s)\", COLORS, ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the approximation is more precise because we sample the states more evenly. (Keep in mind that many environments don't allow for **exploring starts** and we will have to find way to explore the environment from an initial state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    def __init__(self, num_actions: int):\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def act(self, S):\n",
    "        return random.randint(0, self.num_actions - 1)\n",
    "\n",
    "random_agent = RandomAgent(4)\n",
    "\n",
    "V, N = monte_carlo_V(env, random_agent, num_episodes=2000, exploring_starts=True)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "visualize_table([f\"{v:.3f}\" for v in V], \"Value function estimate - Vπ(s)\", COLORS, ax=axs[0])\n",
    "visualize_table([f\"{n}\" for n in N], \"Number of Visits - N(s)\", COLORS, ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even though our policy was not optimal, it is still better than the random one.\n",
    "\n",
    "### State-Action-Value function\n",
    "Although the state-value function is useful, it is of *limited* use in the problems where we don't have the access to the environment **model** (i.e we don't know which actions lead to which states and rewards and with what probability) only after taking the action we know what the next state will be so we cannot use state-value function to choose the best action. (In games like chess and go we know all the rules and can choose the move that leads to the most valued state)\n",
    "\n",
    "This is where the state-action value function comes in. It is denoted as $q_{\\pi}(s,a) = \\mathbf{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a]$ and it is the expected return after taking action a in state s and then following policy $\\pi$. It is clear to see that it would be much more useful in the problems where we don't have the access to the environment model, in each state we will use the action with the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_Q(env: gym.Env, agent: Agent, num_episodes: int=500, gamma: float=0.9, exploring_starts=False, benchmark=False) -> tuple[list, list]:\n",
    "    \"\"\"Monte Carlo algorithm to estimate the state-action value function\"\"\"\n",
    "    Q = [[0.0 for _ in range(4)] for _ in range(env.observation_space.n)] # Set initial estimates to 0\n",
    "    N = [[0   for _ in range(4)] for _ in range(env.observation_space.n)] # Number of visits for each state, action pair\n",
    "\n",
    "    total_rewards = []  # used for benchmarking\n",
    "    for _ in range(num_episodes):\n",
    "        start = 0\n",
    "        action = None\n",
    "\n",
    "        if exploring_starts:\n",
    "            start = np.random.randint(env.observation_space.n)\n",
    "            action = np.random.randint(env.action_space.n)\n",
    "\n",
    "        trajectory = run_episode(env, agent, visualize=False, starting_state=start, starting_action=action)\n",
    "        G = 0\n",
    "        for R, S, A in reversed(trajectory):\n",
    "            if A is not None:\n",
    "                N[S][A] += 1       # Increase the number of visits for state-action pair\n",
    "                Q[S][A] += (G - Q[S][A]) / N[S][A]                 # Update the estimate\n",
    "            G = R + gamma * G\n",
    "\n",
    "        total_reward = sum([R for R, _, _ in trajectory])\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    if benchmark:\n",
    "        return Q, N, sum(total_rewards) / num_episodes\n",
    "    return Q, N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to approximate the state-action value function for the random policy (agent that always takes random actions) - this way we will eventually explore all pairs. Let's run the simulation in **deterministic** environment first (non slippery lake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, N  = monte_carlo_Q(env, random_agent, num_episodes=2000, exploring_starts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def visualize_qtable(Q, ax, color_max=False, round_digits=0, title=\"QTable\"):\n",
    "    # _, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax=ax, bbox=[0, 0, 1, 1])\n",
    "\n",
    "    NSTATES = 16\n",
    "    NACTIONS = 4\n",
    "\n",
    "    width, height = 1.0 / (NACTIONS + 1), 1.0 / (NSTATES + 1)\n",
    "\n",
    "    for s in range(NSTATES):\n",
    "        tb.add_cell(s+1, 0, width, height, text=f\"S{s}\", loc=\"center\", facecolor=\"gray\")\n",
    "        argmax = np.argmax(Q[s])\n",
    "        for a in range(NACTIONS):\n",
    "            if s in TERMINAL:\n",
    "                tb.add_cell(s+1, a+1, width, height, text=\"\", loc=\"center\", facecolor=\"#ccc\")\n",
    "                continue\n",
    "            x = a\n",
    "            y = s\n",
    "            facecolor = \"white\"\n",
    "            if a == argmax and color_max:\n",
    "                facecolor = \"green\"\n",
    "            if round_digits:\n",
    "                text = f\"{Q[s][a]:.{round_digits}f}\"\n",
    "            else:\n",
    "                text = f\"{Q[s][a]}\"\n",
    "            tb.add_cell(y+1, x+1, width, height, text=text, loc=\"center\", facecolor=facecolor)\n",
    "\n",
    "    for a in range(NACTIONS):\n",
    "        tb.add_cell(0, a+1, width, height, text=ARROWS[a], loc=\"center\", facecolor=\"gray\")\n",
    "\n",
    "    ax.add_table(tb)\n",
    "    ax.set_title(title)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))  \n",
    "\n",
    "env.reset()\n",
    "render(env, axs[0])\n",
    "visualize_qtable(Q, axs[1], True, 4)\n",
    "visualize_qtable(N, axs[2], False, 0, \"Number of Visits\")\n",
    "\n",
    "policy = np.argmax(Q, axis=1)\n",
    "policy_arrows = [ARROWS[a] for a in policy]\n",
    "for s in TERMINAL: policy_arrows[s] = \"\"\n",
    "\n",
    "visualize_table(policy_arrows, \"New Policy (greedy with respect to Q)\", COLORS, ncols=4, nrows=4, ax=axs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the Q table is larger than V table. Q table will be much more useful as it stores more information useful for decision making (control problems).\n",
    "\n",
    "For the sake of theory let's show that it is easy to calculate the V function from Q function. \\\n",
    "$ v_{\\pi}(s) = \\sum_{a \\in \\mathit{A}} \\pi(a|s) q_{\\pi}(s,a)$\n",
    "\n",
    "We chose $\\pi$ to be an uniform policy so $\\pi(a|s) = \\frac 1 {|\\mathit{A}|}  = \\frac1 4$ for all actions in each state. \\\n",
    "So the equation simplifies to average of state-action values in each state (mean of each row):\n",
    "\n",
    "Let's see if this is really the case by checking if the V table approximated previously is the same as V calculated from averages in Q table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "visualize_table([f\"{s:.3f}\" for s in V], \"direct approx of V\", COLORS, ax=ax[0])\n",
    "\n",
    "Q_sums = np.mean(Q, axis=1)\n",
    "visualize_table([f\"{s:.3f}\" for s in Q_sums], \"V from approx of Q\", COLORS, ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is indeed the case, the V table is similiar to the one we approximated previously (in the limit it would be exactly the same).\n",
    "\n",
    "Notice that we can also calculate the q function from the v function, but it requires the access to the environment model. \\\n",
    "$ q_{\\pi}(s,a) = \\sum_{s' \\in \\mathit{S}, r \\in \\mathit{R}} p(s',r|s,a) [r + \\gamma v_{\\pi}(s')]$ - where p models dynamics of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy improvement\n",
    "We know how to evaluate the policy, the question is how can we improve it? We can do it by using the value function to choose the best action in each state. This way we can define a new policy that is **greedy** with respect to the **value** function. This way we can guarantee that the new policy will be at least as good as the old one.\n",
    "\n",
    "After calculating new value table for a better policy, all entries should be higher or equal than corresponding values in the old value table. This is called **policy improvement theorem**.\n",
    "\n",
    "Let's try to evaluate the improved policy (greedy with respect to the value function - see previous visualization) and see if it yields higher returns. (We are not using exploring starts here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_visualize(env, agent, num_episodes=1000, exploring_starts=False):\n",
    "    Q, NQ = monte_carlo_Q(env, agent, num_episodes=num_episodes, exploring_starts=exploring_starts)\n",
    "    V, N = monte_carlo_V(env, agent, num_episodes=num_episodes, exploring_starts=exploring_starts)\n",
    "\n",
    "    _, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "    visualize_qtable(Q, axs[0], True, 4)\n",
    "    visualize_qtable(NQ, axs[1])\n",
    "    visualize_table([f\"{v:.3f}\" for v in V], \"Value function estimate - Vπ(s)\", COLORS, ax=axs[2])\n",
    "    visualize_table([f\"{n}\" for n in N], \"Number of Visits - N(s)\", COLORS, ax=axs[3])\n",
    "\n",
    "policy = np.argmax(Q, axis=1)\n",
    "new_agent = DeterministicPolicy(policy)\n",
    "evaluate_and_visualize(env, new_agent, num_episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the new policy is indeed better (in fact it is optimal) however we have a problem with evaluating it. The problem is that the greedy policy is fixed and unable to explore other states than the ones it already knows (it is **exploiting** the knowledge). This is called **exploitation-exploration dilemma**. We need to explore other states to be sure that the policy is indeed optimal. (what happens iw we are dropped in the upper right corner? Is the policy still optimal?)\n",
    "\n",
    "One solution would be to use **exploring starts**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_visualize(env, new_agent, num_episodes=1000, exploring_starts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed we can see that the value is approximated now for all states and state-action pairs. We also see that the policy is better than the random one.\n",
    "\n",
    "Another way would be to use **$\\epsilon$-greedy** policy, it is a way of action where with probability $\\epsilon$ we take a **random** action and with probability $1-\\epsilon$ we take the **greedy** action. This way we can **explore** other states and still **exploit** the knowledge we already have. \n",
    "\n",
    "*(see more in the bandits.ipynb notebook)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(Agent):\n",
    "    def __init__(self, policy, epsilon=0.1):\n",
    "        self.pi = policy\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def act(self, S):\n",
    "        if random.random() < self.epsilon:\n",
    "            return (policy[S] + random.randint(1, 3)) % 4   # Random action different from the greedy one\n",
    "        return policy[S]\n",
    "\n",
    "egreedy_agent = EpsilonGreedyAgent(policy, epsilon=0.1)\n",
    "\n",
    "evaluate_and_visualize(env, egreedy_agent, num_episodes=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having changed the policy from greedy to epsilon greedy we were able to evaluate every position, but of course the visits weren't as balanced in the idelized (**exploring starts**) case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized policy iteration (GPI)\n",
    "The process of iteratively evaluating and improving the policy is called **generalized policy iteration**. It is a general concept that can be used to solve MDPs. It is guaranteed to converge to the optimal policy if we perform both steps infinitely many times. Let's run the GPI algorithm on slippery FrozenLake environment.\n",
    "\n",
    "Similarily as before we will start with a random agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, agent, num_episodes, visualize=True):\n",
    "    Q, _, avg_total_reward = monte_carlo_Q(env, agent, num_episodes=num_episodes, exploring_starts=True, benchmark=True)\n",
    "\n",
    "    if visualize:\n",
    "        fig, ax = plt.subplots()\n",
    "        visualize_qtable(Q, ax, True, 4, \"Qπ(s, a)\")\n",
    "\n",
    "    return Q, avg_total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q0, TR0 = policy_evaluation(senv, random_agent, num_episodes=1000, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's improve the policy greedily with respect to the value function and evaluate it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy1 = np.argmax(Q0, axis=1)\n",
    "agent1 = DeterministicPolicy(policy1)\n",
    "\n",
    "visualize_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one iteration of GPI done, now let's do another one staring by evaluating the new policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1, TR1 = policy_evaluation(senv, agent1, num_episodes=1000, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy2 = np.argmax(Q1, axis=1)\n",
    "agent2 = DeterministicPolicy(policy2)\n",
    "\n",
    "visualize_policy(policy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_agent(env, agent, num_iters=500):\n",
    "    rewards = []\n",
    "    for _ in range(num_iters):\n",
    "        trajectory = run_episode(env, agent, visualize=False)\n",
    "        rewards.append(sum([R for R, _, _ in trajectory]))\n",
    "    return sum(rewards) / len(rewards)\n",
    "\n",
    "print(\"Random Agent: \", benchmark_agent(senv, random_agent))\n",
    "print(\"Policy Agent 1: \", benchmark_agent(senv, agent1))\n",
    "print(\"Policy Agent 2: \", benchmark_agent(senv, agent2))\n",
    "print(\"Policy Agent 3: \", benchmark_agent(senv, agent3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the GPI algorithm and try to run it on a larger FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPI(env, num_episodes, iterations=5):\n",
    "    agent = RandomAgent(4)\n",
    "\n",
    "    Vs = []\n",
    "    Ps = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print(f\"{i} - iteration\")\n",
    "        print(\"\\tEvaluating policy\")\n",
    "        Q = policy_evaluation(env, agent, num_episodes, visualize=False)\n",
    "\n",
    "        V = np.mean(Q, axis=1)\n",
    "        Vs.append(V)\n",
    "\n",
    "        print(\"\\tImproving policy\")\n",
    "        policy = np.argmax(Q, axis=1)\n",
    "\n",
    "        Ps.append(policy)\n",
    "\n",
    "        agent = DeterministicPolicy(policy)\n",
    "\n",
    "    _, axs = plt.subplots(2, iterations, figsize=(5 * iterations, 5))\n",
    "    for i, v in enumerate(Vs):\n",
    "        visualize_table([f\"{s:.3f}\" for s in v], \"Vπ(s)\", ax=axs[0][i])\n",
    "        visualize_policy(Ps[i], ax=axs[1][i], colors=None)\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_env = gym.make(\n",
    "    'FrozenLake-v1',\n",
    "    map_name=\"8x8\", \n",
    "    is_slippery=True,\n",
    "    render_mode='rgb_array'\n",
    ")\n",
    "\n",
    "GPI(big_env, 100, iterations=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "- Value\n",
    "- Bellman equation\n",
    "- Generalized policy iteration\n",
    "- Monte carlo vs Temporal difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- Reinforcement Learning An Introduction (2nd edition) - Richard Sutton and Andrew Barto\n",
    "- Gymnasium framework (https://gymnasium.farama.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
