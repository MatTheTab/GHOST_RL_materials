{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning basics\n",
    "**Reinforcement learning (RL)** is a branch of machine learning where an **agent** learns to make decisions by interacting with an **environment**. It's inspired by how humans and animals learn from trial and error, by taking actions and receiving feedback. In this notebook, we'll introduce the key concepts of reinforcement learning, and apply them to a simple problem.\n",
    "\n",
    "## Agent-Environment interaction\n",
    "We try to frame most RL problems in this format. At each timestep **t** the **agent** receives observation $ O $ which is some representation of environment's **state** $ S_t \\in \\mathit{S}$ and on that basis it selects an **action** $ A_t \\in \\mathit{A(s)} $. One time step later, in part as a consequence of its action, the agent receives a numerical reward, $ R_{t+1} \\in \\mathit{R} \\sub \\mathbb{R} $ , and finds itself in a new state $ S_{t+1} $, together thereby give rise to a sequence or **trajectory** (or *history*) that begins like this: $ S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\ldots $\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "In a finite problem the sets $\\mathit{S}$, $\\mathit{A}$ and $\\mathit{R}$ have finite number of elements.\n",
    "\n",
    "### Example of finite environment\n",
    "One of the simplest environments available in **Gymnasium** framework is **[Frozen Lake (click to read documentation)](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "\n",
    "def render(env: gym.Env):\n",
    "    \"\"\"Helper function to render the game frame.\"\"\"\n",
    "    frame = env.render()\n",
    "    assert frame is not None\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "from matplotlib.table import Table\n",
    "\n",
    "def visualize_table(table_data, title=None):\n",
    "    _, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax=ax, bbox=[0, 0, 1, 1])\n",
    "\n",
    "    nrows, ncols = 4, 4\n",
    "    width, height = 1.0 / ncols, 1.0 / nrows\n",
    "\n",
    "    for i in range(len(table_data)):\n",
    "        x = i % 4\n",
    "        y = i // 4\n",
    "        tb.add_cell(y, x, width, height, text=table_data[i], loc=\"center\", facecolor=\"white\")\n",
    "\n",
    "    ax.add_table(tb)\n",
    "    if title:\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the Frozen Lake environment. We will load 4x4 grid and visualize it. The goal of this game is to reach the present and not fall into the frozen lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    'FrozenLake-v1',\n",
    "    map_name=\"4x4\", \n",
    "    is_slippery=False,\n",
    "    render_mode='rgb_array'\n",
    ")\n",
    "\n",
    "print(\"S: \", env.observation_space)\n",
    "print(\"A: \", env.action_space)\n",
    "print(\"R: \", env.reward_range)\n",
    "\n",
    "O, info = env.reset()\n",
    "\n",
    "print(\"Observation: \", O)\n",
    "render(env)\n",
    "visualize_table(range(16), \"State Space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this is a **finite** environment\n",
    "- State set ($S$) has 16 members (and 5 of them are **terminal** states)\n",
    "- Action set ($A$) has 4 members (left, down, right, up)\n",
    "- Reward set has ($R$) 2 members (1 for reaching the present and 0 for every other move)\n",
    "\n",
    "In this case the environment is a **fully observable** one (observation is equivalent to the state), *however keep in mind that agent doesn't see the board as we do (from the bird view) but only observes on which tile it currently is, if he received reward and if it is a terminal state. The rendering of the board is just for visualization purposes.*\n",
    "\n",
    "Let's see what happens when we take actions in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "S1, R1, T1, _, I1 = env.step(RIGHT)\n",
    "print(\"Observation/State S1: \", S1)\n",
    "print(\"Reward R1: \", R1)\n",
    "print(\"Is terminal T1: \", T1)\n",
    "\n",
    "render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that after taking action $ A_0 = right $ we received the reward $ R_1 = 0 $ and the observation $S_1 = 1$. We also received the information that we are not in a terminal state yet and that this transition had probability of 1 (we are in a **deterministic** environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2, R2, T2, _, I2 = env.step(RIGHT)\n",
    "print(\"Observation/State S2: \", S2)\n",
    "print(\"Reward R2: \", R2)\n",
    "print(\"Is terminal T2: \", T2)\n",
    "render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time after taking action $ A_1 = down $ we received the reward $ R_2 = 0 $ and the observation $S_2 = 5$ but this time we found ourselves in a terminal state as we have fallen into the lake. \n",
    "\n",
    "This is the end of the **episode** with **trajectory** of $ S_0, A_0, R_1, S_1, A_1, R_2, S_2 = 0, right,0,1,down,0,5 $. \\\n",
    "The episode length is usually denoted as **T** and in this case $ T = 2 $ and $S_T = S_2$ is a terminal state. \n",
    "\n",
    "Consider another **trajectory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "actions = [DOWN, DOWN, RIGHT, RIGHT, DOWN, RIGHT]\n",
    "\n",
    "for i, A in enumerate(actions):\n",
    "    O, R, T, _, _ = env.step(A)\n",
    "    print(f\"A{i}: \", A)\n",
    "    print(f\"S{i+1}: \", O)\n",
    "    print(f\"R{i+1}: \", R)\n",
    "    print(\"Is terminal: \", T)\n",
    "    render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can observe that the agent reaches the goal in 6 steps ($T=6$ the terminal state is $S_6$), all the immediate rewards were equal to 0 except the last one $R_6 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "Mathematical formalization of such decision making process is called **Markov Decision Process (MDP)**. The environment is said to have the **Markov property** if the future state depends only on the current state and action, and not on the past **trajectory**. This is a very strong assumption and in practice it's often violated. It helps to simplify the problem as if we learn optimal policies for some states we can hope that it will not change in the future.\n",
    "\n",
    "MDPs can be entirelly defined by the following function: \\\n",
    "$p(s', r | s, a) = Pr(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ \\\n",
    "In other words - *the probability of transitioning to state $s'$ and receiving reward $r$ given that we are in state $s$ and took action $a$ should be defined for all state-action pairs.*\n",
    "\n",
    "The Frozen Lake environment was an example of a **deterministic** MDP, all probabilities were equal to 1 or 0. for example $p(1,0|0,right) = 1$ and $p(1,1|0,right) = 0$. However it is possible to enable **stochastic** transitions by setting the `is_slippery` parameter to `True` in the environment initialization. (Now there is only 1/3 chance that the agent will move in the direction it wanted and 2/3 that it will move into one of orthogonal directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    'FrozenLake-v1',\n",
    "    map_name=\"4x4\", \n",
    "    is_slippery=True,\n",
    "    render_mode='rgb_array'\n",
    ")\n",
    "\n",
    "O, info = env.reset(seed=1)\n",
    "\n",
    "render(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_step(env: gym.Env, A: int, t: int):\n",
    "    O, R, T, _, I = env.step(A)\n",
    "    print(f\"A{t}: {A}, S{t+1}: {O}, R{t+1}: {R}, Terminal: {T}\")\n",
    "    print(\"Info: \", I)\n",
    "    render(env)\n",
    "\n",
    "debug_step(env, DOWN, t=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we performed action $down$ but ended up in the state 1 as if we moved $right$ instead. \n",
    "$1/3$ lead to the desired state and $2/3$ lead to the orthogonal states, but we never slip into the opposite direction, this opens doors for interesting strategies, for example if we want to be sure to avoid the lake we can choose action $up$ to be sure we won't slip into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_step(env, UP, t=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "staying in $S=1$, going back to $S=0$ or going to state $S=2$ all had the same probability (1/3) after choosing action $up$ this time we got lucky and moved closer to the goal. Have we chosen action $right$ we would have 1/3 chance of slipping down to $S=5$ and loosing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals and Reward hypothesis\n",
    "When training RL agent we want it to acomplish some **goal**. The goal is usually some high level objective that is not directly encoded in the environment. For example in the Frozen Lake the goal/purpose is *to reach the goal without falling into the lake*.\n",
    "\n",
    "The **reward hypothesis** states that: \\\n",
    "*All of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received rewards*\n",
    "\n",
    "### Return\n",
    "The cumulative sum of rewards to be received after time step t is called **return** (denoted as: $G_t = R_{t+1} + R_{t+2} + \\ldots + R_T$). To achieve the high level goal we need to maximize the expected return. This way agent might perform actions that in the short term are not beneficial but in the long term lead to the highest return.\n",
    "\n",
    "*Notice that if you define the return of terminal state to 0 ($G_T = 0$) you can formulate the return recursively as: $G_t = R_{t+1} + G_{t+1}$*\n",
    "\n",
    "#### Discounted return\n",
    "In practice we often use **discounted return** where we multiply each reward by a factor $\\gamma^t$ where $\\gamma \\in [0,1]$ is called the discount factor. \n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots + \\gamma^{T-t-1} R_T$\n",
    "This makes sense as we are less likely to predict rewards long in the future than the immediate ones. \\\n",
    "If $\\gamma = 0$ the agent will only care about the immediate reward ($G_t = R_{t+1}$) \\\n",
    "If $\\gamma = 1$ the agent will care about all future rewards equally (*the same as undiscounted case mentioned previously*).\n",
    "\n",
    "The discounted return can be formulated recursively as: $G_t = R_{t+1} + \\gamma G_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise from the book\n",
    "Suppose = 0.5 and the following sequence of rewards is received $R_1 = 1$,\n",
    "$R_2 = 2$, $R_3 = 6$, $R_4 = 3$, and $R_5 = 2$, with $T = 5$. What are $G_0, G_1 , . . ., G_5$?\n",
    "\n",
    "*Hint: Work backwards*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython import display\n",
    "from typing import List, Tuple\n",
    "\n",
    "Trajectory = List[Tuple[int, float, int]]\n",
    "\n",
    "\n",
    "def run_episode(env: gym.Env, actions: list) -> Trajectory:\n",
    "    \"\"\"Run episode by applying list of actions and return list of (state, reward) pairs.\"\"\"\n",
    "    S, I = env.reset()\n",
    "    R = 0.0\n",
    "\n",
    "    frames = []\n",
    "    frames.append(env.render())\n",
    "\n",
    "    trajectory = []\n",
    "    for i, A in enumerate(actions):\n",
    "        trajectory.append((R, S, A))\n",
    "        S, R, T, _, I = env.step(A)\n",
    "        frames.append(env.render())\n",
    "    trajectory.append((R, S, None))\n",
    "\n",
    "    # Animation\n",
    "    fig = plt.figure() \n",
    "\n",
    "    def animate(i):\n",
    "        plt.imshow(frames[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "    ani = FuncAnimation(fig, animate, frames=len(frames))\n",
    "    video = ani.to_html5_video()\n",
    "    html = display.HTML(video)\n",
    "    display.display(html)\n",
    "    plt.close()\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "def discounted_return(t: Trajectory, gamma: float):\n",
    "    \"\"\"Calculate the return of a trajectory.\"\"\"\n",
    "    return sum([R * (gamma ** i) for i, (R, _, _) in enumerate(t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    'FrozenLake-v1',\n",
    "    map_name=\"4x4\", \n",
    "    is_slippery=False,\n",
    "    render_mode='rgb_array'\n",
    ")\n",
    "\n",
    "trajectory = run_episode(env, [RIGHT, RIGHT, DOWN, DOWN, LEFT, DOWN, RIGHT, RIGHT])\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate how return would look like on each state of this episode. We will use multiple discount factors ($\\gamma$) to see how it affects the return. \n",
    "Notice that:\n",
    "- terminal state has always return of 0 as no more rewards can be received after it.\n",
    "- we can only evaluate the return of the states that were visited in the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trajectory(T: Trajectory, gammas=[0, 0.5, 0.9, 1]):\n",
    "    for gamma in gammas:\n",
    "        G_table = [None for _ in range(16)]\n",
    "        G = 0 # Terminal\n",
    "        for R, S, _ in reversed(T):\n",
    "            G_table[S] = G\n",
    "            G = R + gamma * G\n",
    "\n",
    "        G_table = [f\"{G:.2f}\" if G is not None else \"\" for G in G_table]\n",
    "        visualize_table(G_table, f\"Discounted Return (gamma={gamma})\")\n",
    "\n",
    "evaluate_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a more optimal episode, where the agent reaches the goal in a shorter amount of steps, notice that on average returns will be higher than in the longer episode, which is another useful feature of **discounting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = run_episode(env, [DOWN, DOWN, RIGHT, RIGHT, DOWN, RIGHT])\n",
    "evaluate_trajectory(trajectory, gammas=[0.5, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider episode that ends in agent falling into the lake, notice that we never get any reward in such episode so all returns are equal to 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = run_episode(env, [DOWN, DOWN, RIGHT, UP])\n",
    "evaluate_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "- Policy\n",
    "- Value\n",
    "- Bellman equation\n",
    "- Generalized policy iteration\n",
    "- Monte carlo vs Temporal difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- Reinforcement Learning An Introduction (2nd edition) - Richard Sutton and Andrew Barto\n",
    "- Gymnasium framework (https://gymnasium.farama.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
